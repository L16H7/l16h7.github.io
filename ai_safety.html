<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Light Blog</title>

    <!-- Google Fonts - Inconsolata -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="container">
        <header>
            <nav>
                <a href="/" class="site-title">Light Blog</a>
                <div class="nav-links">
                    <a href="./index.html">Home</a>
                </div>
            </nav>
        </header>

        <main>
            <article>
                <h1>AI Safety</h1>
                <div class="article-meta">Jan 19, 2026</div>
                <section>
                    <p>
                        Does AI think humans are good? AI doesn't have emotions or own thoughts. Hence, the question could be phrased as "Does the model learn the pattern that humans are good?" To answer that, we need to look into the training data, the text corpus of the whole internet. If we talk much more about humans being good than humans being bad, the AI would learn that humans are good. After all, it's a statistical intelligence.
                    </p>
                </section>
                <section>
                    <h3>Lie detector for AI</h3>
                    <p>
                        Humans don't always say what's on our mind. Why do you think the AI will always tell (generate tokens) the truth? Well, all they do is predict the next token, right? It's an oversimplification. Like human brains, there are activities happening in their neural architecture, residual streams. Words may lie but neural activations may not. So, if we constantly monitor those, we may be able to catch when AI is trying to deceive us.
                    </p>
                </section>
                <section>
                    <h3>Bit about safety testing</h3>
                    <p>
                        Here is how you should not test. Asking AI what they woud do if we hurt them? Statistically, what do we do when someone hurts us? That's the answer AI will give. You get the idea, right? AI doesn't have agenda or feelings but that doesn't mean they are harmless. Consider this scenario. You have a robot and you abuse it constantly for months? How do you think it will react? One day, it will lose control not because it feels pain or anything but because the robot would match the pattern.
                    </p>
                    <p>
                        Lucky for us that current AI systems don't have long term memory yet. But it's the eventuality. Buckle up! Things are starting to get interesting! Also, be nice to your AI.
                    </p>
                </section>
            </article>
        </main>
    </div>

    <footer>
        <p>&copy; 2026 Light Blog. All rights reserved.</p>
    </footer>
</body>

</html>
